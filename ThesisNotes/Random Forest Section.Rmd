---
title: "Random Forest Section"
output: html_document
---

What is a classification tree and how do you put them together to make a random forest?
can you make a "toy classification bagging routine"? Explain how RF is different

### Classification Trees

Classification trees are a tree-based model and are used to predict a qualitative response. The variables that go into these classification trees can be numerical or categorical. We predict that each observation belongs to the most commonly occuring class (or category) of training observations in the region to which it belongs (James, 2013). They are useful because they provide predictors in situations where there are many variables that interact in complicated, non-linear ways. In interpreting these classification trees, we are often interested in both the class prediction corresponding to a particular terminal node region, and in the class proportions among the training observations that fall into that region. 

So, in simpler terms, a classification tree consists of a set of true/false decision rules. It is kind of like a game of 20 questions, where we ask different questions based on the answers to previous questions, and then at the end we make a guess based on all the answers. We can visualize a decision tree as a set of nodes (corresponding to true/false questions), each of which has two branches depending on the answer to the question. Unlike real trees, we usually draw them with their “root” at the TOP, and the “leaves” at the bottom. In order to make predictions with the tree, we start at the top (the “root” node), and ask questions, traveling left or right in the tree based on what the answer is (left for true and right for false). At each step, we reach a new node, with a new question. Once we reach the bottom (a leaf node), we make a prediction based on the set of answers, just like 20 questions. But unlike 20 questions, the number of questions in a decision tree is not always 20, but can vary.

Shall we look at an example of a classification tree?

We are using a data set from a survey taken in the MAT 111 class (Elementary Probability and Statistics). This data set has 71 rows and 12 variables. The survey includes variables such as sex, height, ideal height, GPA, and the fastest speed ever driven.

```{r, include=FALSE}
library(tree)
library(tigerstats)
library(fastR)

```

```{r}
m111s.tr <- tree(sex~fastest+GPA+height+sleep+weight_feel+love_first,
                 data=m111survey)

summary(m111s.tr)

```


This tree is used to predict the sex of an individual based on the variables of fastest speed ever driven, GPA, height, the amount of sleep the participant got the night before, how the participant feels about their weight, and if the participant believes in love at first sight. The summary given shows the variables actually used in constructing the classification tree, the number of terminal nodes, the residual mean deviance, and the misclassification error rate.

```{r}
plot(m111s.tr)
text(m111s.tr)
```

Looking at this tree, we can see that the first division is set when height is less than 69.5 inches. If the height of an observation is less than 69.5 inches they are put into the left region and those with a height equal to or above 69.5 inches are put into the right region. Those in the left hand region are divided by GPA. If the GPA is greater than or equal to 3.225, the prediction is female. If the GPA is less than 3.225, then a further division by height is made. If the height is less than 66.875 inches, then female is predicted. Otherwise, the sex is predicted as male. Those in the right region are divided by how they feel about their weight, although looking at the two terminal nodes, it appears that it doesn't matter how they feel about their weight; the prediction will still be male. 

As you can see from this example, classification trees are easy to interpret and fairly good predictions can be made from them. In this example the misclassification error rate is about 11.4%, or 8 out of 70 observations were misclassified.

### Bagging

Classification trees suffer from high variance. Variance is the error from sensitivity to small fluctuations in the training set. So,when we say classification trees have high variance, this means that if we split the training data set into two parts at random, and fit a tree to both halves, the results we could get may be very different. On the other hand, a procedure with low variance will yield similar results if applied repeatedly to different data sets (James, 2013). Therefore, classification trees generally do not have the same level of accuracy in predicition as procedures with low variance. However, if we put together many decision trees using the method of bootsrap aggregating (referred to as bagging), then the predictive accuracy can be substantially improved.

Bagging is a "general purpose procedure for reducing the variance of a statistical learning method" (James, 2013). The method uses multiple versions of a training set by using the bootstrap, i.e. sampling with replacement. A random sample of the sample size is taken for each data set. Each of these data sets is used to train a different model. The outputs of the models are combined by averaging (in case of regression) or voting (in case of classification) to create a single output. Bagging is only effective when using unstable (i.e. a small change in the training set can cause a significant change in the model) nonlinear models.

 Decision trees with many levels have high variance and low bias, which means they tend to overfit. In order to reduce variance, and make decision trees more powerful, they are usually combined into “ensembles”. Random forests are one such type of ensemble. 


### Random forests

Random forests are more advanced learning models that are capable of creating more complex decision boundaries than logistic regression  The “forest” part of the name means that it is made up of multiple decision trees(usually, the more trees, the better). Usually in a random forest, instead of building a tree using all the features, we use only a random subset of features and use bagging to create the trees. That means different trees will consider different features when asking questions. But in a forest, other trees would include this feature, so it would still influence the overall prediction by the random forest.

Random forests have low bias (just like individual decision trees), and by adding more trees, we reduce variance.

